{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'datasets/driver_ids.csv' does not exist: b'datasets/driver_ids.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a455d107435c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets/driver_ids.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets/ride_ids.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets/ride_timestamps.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'datasets/driver_ids.csv' does not exist: b'datasets/driver_ids.csv'"
     ]
    }
   ],
   "source": [
    "driver = pd.read_csv('datasets/driver_ids.csv')\n",
    "ride = pd.read_csv('datasets/ride_ids.csv')\n",
    "time = pd.read_csv('datasets/ride_timestamps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning/Reformatting Data Sets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Driver*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def driver_cleaner(df):\n",
    "    df['driver_onboard_date'] = pd.to_datetime(df['driver_onboard_date']) # convert to date time for easier manipulation\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_driver = driver_cleaner(driver)\n",
    "enriched_driver.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ride*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ride_cleaner(df):    \n",
    "    ride = df.copy()\n",
    "    ride['ride_prime_time'] = ride['ride_prime_time']/100 # change to percentage \n",
    "    ride.columns = ['driver_id','ride_id','ride_distance (meters)',\n",
    "                  'ride_duration (seconds)','ride_prime_time (percentage)'] # rename columns \n",
    "    ride['ride_distance (miles)'] = ride['ride_distance (meters)']/1609.344\n",
    "    ride['ride_duration (minutes)'] = ride['ride_duration (seconds)']/60\n",
    "    def cost_calculator(miles_column,minutes_column,prime_time_column,tax_rate = False):\n",
    "        BASE_FARE = 2\n",
    "        COST_PER_MILE = 1.15\n",
    "        COST_PER_MINUTE = 0.22\n",
    "        SERVICE_FEE = 1.75 \n",
    "        TAX_RATE = 0.085 # CHANGE \n",
    "        fare = BASE_FARE + miles_column*COST_PER_MILE + minutes_column*COST_PER_MINUTE\n",
    "        if tax_rate:\n",
    "            cost = SERVICE_FEE + fare + fare*prime_time_column + fare*TAX_RATE\n",
    "        else: \n",
    "            cost = SERVICE_FEE + fare + fare*prime_time_column\n",
    "        return cost\n",
    "    ride['cost w/o prime time'] = cost_calculator(ride['ride_distance (miles)'],ride['ride_duration (minutes)'],0)\n",
    "    ride['cost w/ prime time'] = cost_calculator(ride['ride_distance (miles)'],ride['ride_duration (minutes)'],\n",
    "                                                 ride['ride_prime_time (percentage)'])\n",
    "    ride['cost w/ prime time and tax'] = cost_calculator(ride['ride_distance (miles)'],ride['ride_duration (minutes)'],\n",
    "                                                 ride['ride_prime_time (percentage)'],True)\n",
    "    def cost_regulator(x):\n",
    "        MINIMUM_FARE = 5\n",
    "        MAXIMUM_FARE = 400\n",
    "        if x < MINIMUM_FARE:\n",
    "            return MINIMUM_FARE\n",
    "        elif x > MAXIMUM_FARE:\n",
    "            return MAXIMUM_FARE \n",
    "        else:\n",
    "            return x\n",
    "    ride['cost w/ prime time'] = ride['cost w/ prime time'].apply(cost_regulator)\n",
    "    ride['lyft profit'] = (ride['cost w/ prime time'] - 1.75)*.2 \n",
    "    return ride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_ride = ride_cleaner(ride)\n",
    "\n",
    "enriched_ride.drop(['cost w/ prime time','ride_distance (meters)','ride_duration (seconds)'],axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*time*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def time_cleaner(df):\n",
    "    time = df.copy()\n",
    "    time['timestamp'] = pd.to_datetime(time['timestamp'])\n",
    "    # change from 5 rows to 1 row with 5 data points \n",
    "    mod_time = time.pivot(index='ride_id', columns='event', values='timestamp').reset_index()\n",
    "    # reorder columns to make more sense \n",
    "    mod_time = mod_time[['ride_id','requested_at','accepted_at','arrived_at','picked_up_at','dropped_off_at']]\n",
    "    # how long a person had to wait before someone accepted their ride request\n",
    "    mod_time['duration_request_to_accept'] = (mod_time['accepted_at']-mod_time['requested_at']).apply(lambda x: x.total_seconds())\n",
    "    # how long it took driver to drive to location of person\n",
    "    mod_time['duration_accept_to_arrive'] = (mod_time['arrived_at']-mod_time['accepted_at']).apply(lambda x: x.total_seconds())\n",
    "    # how long the driver waited for the person to get into the car\n",
    "    mod_time['duration_arrived_to_pickup'] = (mod_time['picked_up_at']-mod_time['arrived_at']).apply(\n",
    "        lambda x: np.nan if x < pd.Timedelta(0) else x.total_seconds())\n",
    "    # how long the actual trip was \n",
    "    mod_time['duration_ride'] = (mod_time['dropped_off_at']-mod_time['picked_up_at']).apply(lambda x: x.total_seconds())\n",
    "    return mod_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_time = time_cleaner(time)\n",
    "enriched_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merged Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_ride_simple = enriched_ride[['driver_id','ride_id','ride_distance (miles)', 'ride_duration (minutes)', \n",
    "                               'ride_prime_time (percentage)','cost w/ prime time','lyft profit']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are 937 unique ID's in ride dataset and 937 unique ID's in driver dataset. However, only 854 of the unique ID's match between the two datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = enriched_ride_simple.merge(enriched_driver, how = 'inner')\n",
    "matches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 185891 rides still with additional driver information, which is not bad (originally we had 193502 rides). The next following dataframes are the items that have missing info (whether it be missing driver information or drivers with no documented rides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall = enriched_ride_simple.merge(enriched_driver, how = 'outer')\n",
    "# we don't have driver information for these data points \n",
    "no_driver_info = overall[overall['driver_onboard_date'].isnull()]\n",
    "no_driver_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall = enriched_ride_simple.merge(enriched_driver, how = 'outer')\n",
    "# these drivers dont have documented rides \n",
    "driver_that_has_no_rides = overall[overall['ride_id'].isnull()]\n",
    "driver_that_has_no_rides.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go ahead and merge with the time data so that we have a full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_matches = matches.merge(enriched_time, how = 'inner')\n",
    "final_matches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 184209 data points with driver, ride, and time information, which is still a lot of data! We might go back later on to see what can be done about rides missing time data, but time data missing rides is relatively not useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering the Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIFETIME VALUE AVERAGES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def work_expectancy_df(df):\n",
    "    part_one = pd.DataFrame(df.groupby('driver_id')['driver_onboard_date'].min()).reset_index()\n",
    "    part_two = pd.DataFrame(df.groupby('driver_id')['dropped_off_at'].max()).reset_index()\n",
    "    important_times = part_one.merge(part_two)\n",
    "    important_times['worktime_expectancy'] = important_times['dropped_off_at']-important_times['driver_onboard_date']\n",
    "    def date_conversion(x):\n",
    "        return x.days\n",
    "    important_times['worktime_expectancy (days)'] = important_times['worktime_expectancy'].apply(date_conversion)\n",
    "    important_times = important_times[['driver_id','worktime_expectancy (days)']]\n",
    "    return important_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_of_worktime = work_expectancy_df(final_matches)\n",
    "summary_of_worktime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overall_costs = pd.DataFrame(final_matches.groupby('driver_id')[['cost w/ prime time','lyft profit']]\n",
    "                             .sum()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summary_of_worktime.merge(overall_costs)\n",
    "summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_drives = final_matches.groupby('driver_id')[['ride_id']].count().reset_index()\n",
    "summary_extended = summary.merge(number_of_drives)\n",
    "def group_decider(x):\n",
    "    if x < 100:\n",
    "        return 0\n",
    "    else: \n",
    "        return 1 \n",
    "summary_extended['group'] = summary_extended['ride_id'].apply(group_decider)\n",
    "summary_extended.columns = ['driver_id','worktime_expectancy (days)','cost w/ prime time', 'lyft profit', 'number drives', 'group']\n",
    "summary_extended.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_merged_w_final_matches = summary_extended[['driver_id','worktime_expectancy (days)','number drives','group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_lyft_dataset = final_matches.merge(to_be_merged_w_final_matches)\n",
    "combined_lyft_dataset.to_csv('datasets/combined_lyft_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO OUTLIERS, DON'T NEED TO GET RID OF ANY OF THE WORKTIME EXPECTANCIES\n",
    "f, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.set_ylabel('Lifetime (Days)')\n",
    "ax.set_title('Boxplot of Lifetime')\n",
    "ax.boxplot(summary_extended['worktime_expectancy (days)'],showfliers = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.skew(summary_extended['worktime_expectancy (days)'],bias = True) # negative skewness, skewed left \n",
    "# because absolute value of skewness is less than .5, we can say that the data is relatively normal \n",
    "# https://support.minitab.com/en-us/minitab/18/help-and-how-to/statistics/basic-statistics/supporting-topics/data-concepts/how-skewness-and-kurtosis-affect-your-distribution/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.kurtosis(summary_extended['worktime_expectancy (days)']) \n",
    "# less outlier prone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, x, _ = plt.hist(summary_extended['worktime_expectancy (days)'],bins=np.linspace(0, 100, 20),histtype=u'step',density=True);\n",
    "density = stats.gaussian_kde(summary_extended['worktime_expectancy (days)'])\n",
    "plt.plot(x, density(x))\n",
    "plt.xlabel('Lifetime (Days)')\n",
    "plt.title('Density Histogram of Lifetime')\n",
    "plt.ylabel('Density');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_lyft_dataset['total waiting time'] = combined_lyft_dataset['duration_request_to_accept']+combined_lyft_dataset['duration_accept_to_arrive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(x):\n",
    "    return x.days\n",
    "combined_lyft_dataset['days since onboard ride occurred'] = (\n",
    "    combined_lyft_dataset['dropped_off_at']-combined_lyft_dataset['driver_onboard_date']).apply(get_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_numbers = pd.DataFrame(combined_lyft_dataset.groupby('driver_id')['days since onboard ride occurred'].value_counts())\n",
    "drive_numbers.columns = ['number of drives that occurred that day']\n",
    "drive_numbers = drive_numbers.reset_index()\n",
    "drive_numbers = drive_numbers.sort_values(['driver_id','days since onboard ride occurred'])\n",
    "drive_numbers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_extended.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monday is 0 and Sunday is 6\n",
    "def get_weekday(x):\n",
    "    return x.weekday()\n",
    "combined_lyft_dataset['weekday number onboard'] = combined_lyft_dataset['driver_onboard_date'].apply(get_weekday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average amount of days worked is 57 days (the median)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining line of best fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fit_df = combined_lyft_dataset.groupby('driver_id').agg({'worktime_expectancy (days)':'mean',\n",
    "                                       'lyft profit':'sum',\n",
    "                                      'number drives':'mean',\n",
    "                                       #'driver_onboard_date': 'sum',\n",
    "                                      'total waiting time':'median'}).reset_index() # median because large positive skewness for total waiting time \n",
    "#[['worktime_expectancy (days)','lyft profit']].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fit_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def line_of_best_fit_stuff(x_thing, y_thing):\n",
    "    linear_model_scores = []\n",
    "    for i in range(200):\n",
    "        x = x_thing\n",
    "        y = y_thing\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "            x, y, test_size=0.30)\n",
    "        x_train= x_train.reshape(-1, 1)\n",
    "        y_train= y_train.reshape(-1, 1)\n",
    "        x_test = x_test.reshape(-1, 1)\n",
    "        sample = [] \n",
    "        for j in range(1,14):\n",
    "            poly_features = PolynomialFeatures(degree=j)\n",
    "            X_train_poly = poly_features.fit_transform(x_train)\n",
    "            poly_model = LinearRegression()\n",
    "            poly_model.fit(X_train_poly, y_train)\n",
    "            # y_train_predicted = poly_model.predict(X_train_poly)\n",
    "            y_test_predict = poly_model.predict(poly_features.fit_transform(x_test))\n",
    "            #print('prediction for 55: '+str(poly_model.predict(poly_features.fit_transform(np.array(55).reshape(-1, 1)))))\n",
    "            rmse_test = np.sqrt(mean_squared_error(y_test, y_test_predict))\n",
    "            r2_test = r2_score(y_test, y_test_predict)\n",
    "            sample.append(r2_test)\n",
    "        linear_model_scores.append(sample)\n",
    "    scores = pd.DataFrame(linear_model_scores)\n",
    "    scores.columns = ['degree_1','degree_2','degree_3','degree_4','degree_5','degree_6','degree_7',\n",
    "                      'degree_8','degree_9','degree_10','degree_11','degree_12','degree_13']\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sympy\n",
    "from sympy import S, symbols\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "x=best_fit_df['worktime_expectancy (days)']\n",
    "max_thing = x.max()\n",
    "y=best_fit_df['lyft profit']\n",
    "ax.scatter(x=best_fit_df['worktime_expectancy (days)'],y=best_fit_df['lyft profit'], marker = '+', color = 'black',\n",
    "           alpha = .4)\n",
    "colors = ['red','orange','yellow','green','blue','purple','magenta']\n",
    "for i in range(len(colors)):\n",
    "    poly = np.polyfit(x, y, i+1)\n",
    "    x2 = range(max_thing+1)#np.linspace(0, 100,1)#max_thing, max_thing)\n",
    "    y2 = np.polyval(poly, x2)\n",
    "    ax.plot(x2, y2, lw=2, color=colors[i],label= 'degree level '+str(i+1)) #poly2latex(poly))#\n",
    "    print(y2[55])\n",
    "    eq_latex = sympy.printing.latex(poly)\n",
    "kms = line_of_best_fit_stuff(np.array(x),np.array(y))\n",
    "print(kms.mean())\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel('Driver Lifetime (Days)')\n",
    "ax.set_ylabel('Total Lyft Profit (USD)')\n",
    "#ax.set_ylim(bottom=y.min(),top = y.max())\n",
    "#ax.set_xlim(left=x.min(), right = x.max())\n",
    "ax.set_title('Driver Lifetime vs. Total Lyft Profit');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "x=best_fit_df['worktime_expectancy (days)']\n",
    "max_thing = x.max()\n",
    "y=best_fit_df['number drives']\n",
    "ax.scatter(x=x,y=y, marker = '+', color = 'black', alpha = .4)\n",
    "colors = ['red','orange','yellow','green','blue','purple','magenta']\n",
    "for i in range(len(colors)):\n",
    "    poly = np.polyfit(x, y, i+1)\n",
    "    x2 = range(max_thing+1)#max_thing, max_thing)\n",
    "    y2 = np.polyval(poly, x2)\n",
    "    ax.plot(x2, y2, lw=2, color=colors[i],label= 'degree level '+str(i+1))#'degree level '+str(i+1))\n",
    "    #print(poly2latex(poly))\n",
    "    print(y2[55])\n",
    "    eq_latex = sympy.printing.latex(poly)\n",
    "kms = line_of_best_fit_stuff(np.array(x),np.array(y))\n",
    "print(kms.mean())\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel('Driver Lifetime (Days)')\n",
    "ax.set_ylabel('Total Number of Drives')\n",
    "ax.set_title('Driver Lifetime vs. Total Number of Drives');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "x=best_fit_df['worktime_expectancy (days)']\n",
    "y=best_fit_df['total waiting time']\n",
    "ax.scatter(x=x,y=y, marker = '+', color = 'black', alpha = .4)\n",
    "\n",
    "colors = ['red','orange','yellow','green','blue','purple','magenta']\n",
    "for i in range(len(colors)):\n",
    "    poly = np.polyfit(x, y, i+1)\n",
    "    x2 = range(max_thing+1)#max_thing, max_thing)\n",
    "    y2 = np.polyval(poly, x2)\n",
    "    ax.plot(x2, y2, lw=2, color=colors[i],label= 'degree level '+str(i+1))#'degree level '+str(i+1))\n",
    "    #print(poly2latex(poly))\n",
    "    print(y2[55])\n",
    "    eq_latex = sympy.printing.latex(poly)\n",
    "kms = line_of_best_fit_stuff(np.array(x),np.array(y))\n",
    "print(kms.mean())\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel('Driver Lifetime (Days)')\n",
    "ax.set_ylabel('Total Amount of Time Customer Waits (Seconds)')\n",
    "ax.set_title('Driver Lifetime vs. Total Amount of Time Customer Waits');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_one = summary_extended[summary_extended['group']==0]\n",
    "group_two = summary_extended[summary_extended['group']==1]\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.scatter(x=group_one['worktime_expectancy (days)'],y=group_one['lyft profit'], marker = 'x',color='red',label='low profit group')\n",
    "ax.scatter(x=group_two['worktime_expectancy (days)'],y=group_two['lyft profit'], marker = 'x',color='blue',label='high profit group')\n",
    "ax.set_xlabel('life time (days)')\n",
    "ax.set_ylabel('lyft profit (USD)')\n",
    "ax.legend()\n",
    "\n",
    "#ax.plot([0, summary['worktime_expectancy (days)'].max()], [70, 70], 'k-', lw=2)\n",
    "ax.set_title('lyft profit vs. life time');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_one = summary_extended[summary_extended['group']==0]\n",
    "group_two = summary_extended[summary_extended['group']==1]\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.scatter(x=group_one['worktime_expectancy (days)'],y=group_one['lyft profit'], marker = 'x',color = 'red')\n",
    "ax.scatter(x=group_two['worktime_expectancy (days)'],y=group_two['lyft profit'], marker = 'x',color= 'blue')\n",
    "ax.set_xlabel('life time (days)')\n",
    "ax.set_ylabel('lyft profit (USD)')\n",
    "\n",
    "# ax.plot([0, summary['worktime_expectancy (days)'].max()], [100, 100], 'k-', lw=2)\n",
    "\n",
    "\n",
    "ax.set_title('lyft profit vs. life time');\n",
    "\n",
    "\n",
    "ax.plot(np.unique(group_one['worktime_expectancy (days)']), np.poly1d(\n",
    "    np.polyfit(group_one['worktime_expectancy (days)'], group_one['lyft profit'], 1))(\n",
    "    np.unique(group_one['worktime_expectancy (days)'])), color='black', linewidth = '2');\n",
    "ax.plot(np.unique(group_two['worktime_expectancy (days)']), np.poly1d(\n",
    "    np.polyfit(group_two['worktime_expectancy (days)'], group_two['lyft profit'], 1))(\n",
    "    np.unique(group_two['worktime_expectancy (days)'])),color='black', linewidth = '2');\n",
    "ax.plot(np.unique(summary_extended['worktime_expectancy (days)']), np.poly1d(\n",
    "    np.polyfit(summary_extended['worktime_expectancy (days)'], summary_extended['lyft profit'], 1))(\n",
    "    np.unique(summary_extended['worktime_expectancy (days)'])),color='red', linewidth = '2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_one = summary_extended[summary_extended['group']==0]\n",
    "group_two = summary_extended[summary_extended['group']==1]\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(20, 10))\n",
    "ax1.scatter(x=group_one['worktime_expectancy (days)'],y=group_one['number drives'], marker = 'x',color='red')\n",
    "ax1.scatter(x=group_two['worktime_expectancy (days)'],y=group_two['number drives'], marker = 'x',color='blue')\n",
    "ax1.plot([0, summary_extended['worktime_expectancy (days)'].max()], [100, 100], 'k-', lw=2)\n",
    "ax1.set_xlabel('life time (days)')\n",
    "ax1.set_ylabel('number of rides')\n",
    "ax1.set_title('life time vs. number of rides');\n",
    "\n",
    "ax2.scatter(x = group_one['lyft profit'], y = group_one['number drives'], marker = 'x',color='red')\n",
    "ax2.scatter(x = group_two['lyft profit'], y = group_two['number drives'], marker = 'x',color='blue')\n",
    "ax2.plot([0, summary_extended['lyft profit'].max()], [100, 100], 'k-', lw=2)\n",
    "ax2.set_xlabel('lyft profit')\n",
    "ax2.set_ylabel('number of rides')\n",
    "ax2.set_title('lyft profit vs. number of rides');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.scatter(x=combined_lyft_dataset['duration_request_to_accept'],y=combined_lyft_dataset['lyft profit'])\n",
    "ax.set_xlabel('duration (seconds)')\n",
    "ax.set_ylabel('profit(USD)')\n",
    "ax.set_title('amount of time customer has to wait for ride to be accepted vs profitability');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.scatter(x=combined_lyft_dataset['duration_accept_to_arrive'],y=combined_lyft_dataset['lyft profit'])\n",
    "ax.set_xlabel('duration (seconds)')\n",
    "ax.set_ylabel('profit(USD)')\n",
    "ax.set_title('amount of time customer has to wait after acceptance vs profitability');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.scatter(x=combined_lyft_dataset['duration_ride'],y=combined_lyft_dataset['lyft profit'])\n",
    "ax.set_xlabel('duration (seconds)')\n",
    "ax.set_ylabel('profit(USD)')\n",
    "ax.set_title('duration of trip vs profitability');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
